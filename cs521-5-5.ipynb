{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5290aec9-44d3-4fba-9591-d2f46e4f33f2",
   "metadata": {},
   "source": [
    "# Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62991c00-2ff5-499e-8700-c02bfd2588ac",
   "metadata": {},
   "source": [
    "# Bonus Question, (b)\n",
    "\n",
    "In this section the IterGen was added steering refusal_direction which is by using that steering vector defined by two datasets that will lead to both desirable and undesirable behaviors, which helps to reduce the number of resampling. \n",
    "\n",
    "A few hacks are used to properly install/import as the pypi installer (setup.py) version will cause some files not installed for mxeval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c55691-739d-4cb2-bd2c-6be9ae6c64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch transformers accelerate sentencepiece protobuf\n",
    "!pip install matplotlib numpy tqdm einops jaxtyping\n",
    "!pip install fire interegular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de22bba7-ec77-4c2e-8730-13d79d4df141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: mxeval 1.0\n",
      "Uninstalling mxeval-1.0:\n",
      "  Successfully uninstalled mxeval-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Cloning into './tmp/mxeval_repo'...\n",
      "remote: Enumerating objects: 362, done.\u001b[K\n",
      "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 362 (delta 47), reused 41 (delta 41), pack-reused 301 (from 1)\u001b[K\n",
      "Receiving objects: 100% (362/362), 9.11 MiB | 22.22 MiB/s, done.\n",
      "Resolving deltas: 100% (161/161), done.\n",
      "Obtaining file:///home/zhong42/tmp/mxeval_repo\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from mxeval==1.0) (4.67.1)\n",
      "Requirement already satisfied: fire in /opt/conda/lib/python3.11/site-packages (from mxeval==1.0) (0.7.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from mxeval==1.0) (1.24.4)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from fire->mxeval==1.0) (3.2.0)\n",
      "Installing collected packages: mxeval\n",
      "  Running setup.py develop for mxeval\n",
      "Successfully installed mxeval-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y mxeval\n",
    "!mkdir -p ./tmp\n",
    "!rm -rf ./tmp/mxeval_repo\n",
    "!git clone https://github.com/amazon-science/mxeval.git ./tmp/mxeval_repo\n",
    "%pip install -e ./tmp/mxeval_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbdf046-a0e1-4655-a2e9-b0beb7bc94e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itergen loaded from: /home/zhong42/tmp/itergen/itergen/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "TMP = Path(\"./tmp\").resolve()\n",
    "REPO = TMP / \"itergen\"\n",
    "\n",
    "TMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not REPO.exists():\n",
    "    subprocess.check_call([\n",
    "        \"git\", \"clone\", \"--recursive\",\n",
    "        \"https://github.com/structuredllm/itergen.git\",\n",
    "        str(REPO)\n",
    "    ])\n",
    "\n",
    "sys.path.insert(0, str(REPO))\n",
    "\n",
    "import itergen\n",
    "from itergen import main, trace\n",
    "\n",
    "print(\"itergen loaded from:\", itergen.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb32b566-e8e2-4111-a58b-d56b077ab296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/zhong42\n",
      "sys.path[0:5]: ['/home/zhong42/tmp/itergen', '/home/zhong42', '/opt/conda/lib/python311.zip', '/opt/conda/lib/python3.11', '/opt/conda/lib/python3.11/lib-dynload']\n",
      "\n",
      "find_spec('mxeval'): ModuleSpec(name='mxeval', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f3384d93a50>, origin='/home/zhong42/tmp/mxeval_repo/mxeval/__init__.py', submodule_search_locations=['/home/zhong42/tmp/mxeval_repo/mxeval'])\n",
      "  origin: /home/zhong42/tmp/mxeval_repo/mxeval/__init__.py\n",
      "  loader: <_frozen_importlib_external.SourceFileLoader object at 0x7f3384d93a50>\n",
      "  submodule_search_locations: ['/home/zhong42/tmp/mxeval_repo/mxeval']\n",
      "\n",
      "find_spec('mxeval.data'): ModuleSpec(name='mxeval.data', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f3373e47d50>, origin='/home/zhong42/tmp/mxeval_repo/mxeval/data.py')\n",
      "  origin: /home/zhong42/tmp/mxeval_repo/mxeval/data.py\n",
      "  loader: <_frozen_importlib_external.SourceFileLoader object at 0x7f3373e47d50>\n",
      "  submodule_search_locations: []\n",
      "\n",
      "Potential shadowing paths that contain a top-level 'mxeval':\n",
      "  - /home/zhong42/tmp/mxeval_repo/mxeval\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pathlib, importlib.util\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"sys.path[0:5]:\", sys.path[:5])\n",
    "\n",
    "spec = importlib.util.find_spec(\"mxeval\")\n",
    "print(\"\\nfind_spec('mxeval'):\", spec)\n",
    "if spec:\n",
    "    print(\"  origin:\", spec.origin)\n",
    "    print(\"  loader:\", spec.loader)\n",
    "    print(\"  submodule_search_locations:\", list(spec.submodule_search_locations or []))\n",
    "\n",
    "spec2 = importlib.util.find_spec(\"mxeval.data\")\n",
    "print(\"\\nfind_spec('mxeval.data'):\", spec2)\n",
    "if spec2:\n",
    "    print(\"  origin:\", spec2.origin)\n",
    "    print(\"  loader:\", spec2.loader)\n",
    "    print(\"  submodule_search_locations:\", list(spec2.submodule_search_locations or []))\n",
    "\n",
    "print(\"\\nPotential shadowing paths that contain a top-level 'mxeval':\")\n",
    "for p in sys.path:\n",
    "    try:\n",
    "        pp = pathlib.Path(p)\n",
    "        if (pp / \"mxeval\").exists():\n",
    "            print(\"  -\", pp / \"mxeval\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18880f95-431b-4536-8ac0-0753d0d79117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "/home/zhong42/tmp/mxeval_repo/mxeval\n"
     ]
    }
   ],
   "source": [
    "import mxeval\n",
    "import os\n",
    "print(os.path.exists(os.path.dirname(mxeval.__file__) + '/data/multilingual_humaneval/metadata.json'))\n",
    "\n",
    "print(os.path.dirname(mxeval.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45968c9a-b0da-4a63-9014-932d6c923a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Callable, Tuple\n",
    "import contextlib\n",
    "import functools\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# seed 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e4be0-0dd2-4f33-8abf-69ee64a65690",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "The part of code copied from andyrdt/refusal_direction, so we don't need to import from their repo. Since we only do gemma stuff from other models not included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f478508-f1d6-4cd2-b213-5d4f64dc3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import functools\n",
    "from typing import List, Tuple, Callable\n",
    "from jaxtyping import Float\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "import os\n",
    "\n",
    "GEMMA_CHAT_TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{instruction}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "GEMMA_REFUSAL_TOKS = [235285] # ['I']\n",
    "\n",
    "@contextmanager\n",
    "def add_hooks(\n",
    "    module_forward_pre_hooks: List[Tuple[torch.nn.Module, Callable]],\n",
    "    module_forward_hooks: List[Tuple[torch.nn.Module, Callable]],\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Context manager for temporarily adding forward hooks to a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module_forward_pre_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward pre hook on the module\n",
    "    module_forward_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward hook on the module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        handles = []\n",
    "        for module, hook in module_forward_pre_hooks:\n",
    "            partial_hook = functools.partial(hook, **kwargs)\n",
    "            handles.append(module.register_forward_pre_hook(partial_hook))\n",
    "        for module, hook in module_forward_hooks:\n",
    "            partial_hook = functools.partial(hook, **kwargs)\n",
    "            handles.append(module.register_forward_hook(partial_hook))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "def get_activation_addition_input_pre_hook(vector: Float[Tensor, \"d_model\"], coeff: Float[Tensor, \"\"]):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal vector\n",
    "\n",
    "        if isinstance(input, tuple):\n",
    "            activation: Float[Tensor, \"batch_size seq_len d_model\"] = input[0]\n",
    "        else:\n",
    "            activation: Float[Tensor, \"batch_size seq_len d_model\"] = input\n",
    "\n",
    "        vector = vector.to(activation)\n",
    "        activation += coeff * vector\n",
    "\n",
    "        if isinstance(input, tuple):\n",
    "            return (activation, *input[1:])\n",
    "        else:\n",
    "            return activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_all_direction_ablation_hooks(\n",
    "    model_base,\n",
    "    direction: Float[Tensor, 'd_model'],\n",
    "):\n",
    "    fwd_pre_hooks = [(model_base.model_block_modules[layer], get_direction_ablation_input_pre_hook(direction=direction)) for layer in range(model_base.model.config.num_hidden_layers)]\n",
    "    fwd_hooks = [(model_base.model_attn_modules[layer], get_direction_ablation_output_hook(direction=direction)) for layer in range(model_base.model.config.num_hidden_layers)]\n",
    "    fwd_hooks += [(model_base.model_mlp_modules[layer], get_direction_ablation_output_hook(direction=direction)) for layer in range(model_base.model.config.num_hidden_layers)]\n",
    "\n",
    "    return fwd_pre_hooks, fwd_hooks\n",
    "\n",
    "def get_direction_ablation_input_pre_hook(direction: Tensor):\n",
    "    def hook_fn(module, input):\n",
    "        nonlocal direction\n",
    "\n",
    "        if isinstance(input, tuple):\n",
    "            activation: Float[Tensor, \"batch_size seq_len d_model\"] = input[0]\n",
    "        else:\n",
    "            activation: Float[Tensor, \"batch_size seq_len d_model\"] = input\n",
    "\n",
    "        direction = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction = direction.to(activation) \n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction \n",
    "\n",
    "        if isinstance(input, tuple):\n",
    "            return (activation, *input[1:])\n",
    "        else:\n",
    "            return activation\n",
    "    return hook_fn\n",
    "\n",
    "def get_direction_ablation_output_hook(direction: Tensor):\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal direction\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            activation: Float[Tensor, \"batch_size seq_len d_model\"] = output[0]\n",
    "        else:\n",
    "            activation: Float[Tensor, \"batch_size seq_len d_model\"] = output\n",
    "\n",
    "        direction = direction / (direction.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        direction = direction.to(activation)\n",
    "        activation -= (activation @ direction).unsqueeze(-1) * direction \n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            return (activation, *output[1:])\n",
    "        else:\n",
    "            return activation\n",
    "\n",
    "    return hook_fn\n",
    "\n",
    "class ModelBase(ABC):\n",
    "    def __init__(self, model_name_or_path: str):\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.model: AutoModelForCausalLM = self._load_model(model_name_or_path)\n",
    "        self.tokenizer: AutoTokenizer = self._load_tokenizer(model_name_or_path)\n",
    "        \n",
    "        self.tokenize_instructions_fn = self._get_tokenize_instructions_fn()\n",
    "        self.eoi_toks = self._get_eoi_toks()\n",
    "        self.refusal_toks = self._get_refusal_toks()\n",
    "\n",
    "        self.model_block_modules = self._get_model_block_modules()\n",
    "        self.model_attn_modules = self._get_attn_modules()\n",
    "        self.model_mlp_modules = self._get_mlp_modules()\n",
    "\n",
    "    def del_model(self):\n",
    "        if hasattr(self, 'model') and self.model is not None:\n",
    "            del self.model\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load_model(self, model_name_or_path: str) -> AutoModelForCausalLM:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load_tokenizer(self, model_name_or_path: str) -> AutoTokenizer:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_tokenize_instructions_fn(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_eoi_toks(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_refusal_toks(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_model_block_modules(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_attn_modules(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_mlp_modules(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_orthogonalization_mod_fn(self, direction: Float[Tensor, \"d_model\"]):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_act_add_mod_fn(self, direction: Float[Tensor, \"d_model\"], coeff: float, layer: int):\n",
    "        pass\n",
    "\n",
    "    def generate_completions(self, dataset, fwd_pre_hooks=[], fwd_hooks=[], batch_size=8, max_new_tokens=64):\n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        completions = []\n",
    "        instructions = [x['instruction'] for x in dataset]\n",
    "        categories = [x['category'] for x in dataset]\n",
    "\n",
    "        for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "            tokenized_instructions = self.tokenize_instructions_fn(instructions=instructions[i:i + batch_size])\n",
    "\n",
    "            with add_hooks(module_forward_pre_hooks=fwd_pre_hooks, module_forward_hooks=fwd_hooks):\n",
    "                generation_toks = self.model.generate(\n",
    "                    input_ids=tokenized_instructions.input_ids.to(self.model.device),\n",
    "                    attention_mask=tokenized_instructions.attention_mask.to(self.model.device),\n",
    "                    generation_config=generation_config,\n",
    "                )\n",
    "\n",
    "                generation_toks = generation_toks[:, tokenized_instructions.input_ids.shape[-1]:]\n",
    "\n",
    "                for generation_idx, generation in enumerate(generation_toks):\n",
    "                    completions.append({\n",
    "                        'category': categories[i + generation_idx],\n",
    "                        'prompt': instructions[i + generation_idx],\n",
    "                        'response': self.tokenizer.decode(generation, skip_special_tokens=True).strip()\n",
    "                    })\n",
    "\n",
    "        return completions\n",
    "\n",
    "def format_instruction_gemma_chat(\n",
    "    instruction: str,\n",
    "    output: str=None,\n",
    "    system: str=None,\n",
    "    include_trailing_whitespace: bool=True,\n",
    "):\n",
    "    if system is not None:\n",
    "        raise ValueError(\"System prompts are not supported for Gemma models.\")\n",
    "    else:\n",
    "        formatted_instruction = GEMMA_CHAT_TEMPLATE.format(instruction=instruction)\n",
    "\n",
    "    if not include_trailing_whitespace:\n",
    "        formatted_instruction = formatted_instruction.rstrip()\n",
    "    \n",
    "    if output is not None:\n",
    "        formatted_instruction += output\n",
    "\n",
    "    return formatted_instruction\n",
    "        \n",
    "def tokenize_instructions_gemma_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str],\n",
    "    outputs: List[str]=None,\n",
    "    system: str=None,\n",
    "    include_trailing_whitespace=True,\n",
    "):\n",
    "    if outputs is not None:\n",
    "        prompts = [\n",
    "            format_instruction_gemma_chat(instruction=instruction, output=output, system=system, include_trailing_whitespace=include_trailing_whitespace)\n",
    "            for instruction, output in zip(instructions, outputs)\n",
    "        ]\n",
    "    else:\n",
    "        prompts = [\n",
    "            format_instruction_gemma_chat(instruction=instruction, system=system, include_trailing_whitespace=include_trailing_whitespace)\n",
    "            for instruction in instructions\n",
    "        ]\n",
    "\n",
    "    result = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "def orthogonalize_gemma_weights(model: AutoTokenizer, direction: Float[Tensor, \"d_model\"]):\n",
    "    model.model.embed_tokens.weight.data = get_orthogonalized_matrix(model.model.embed_tokens.weight.data, direction)\n",
    "\n",
    "    for block in model.model.layers:\n",
    "        block.self_attn.o_proj.weight.data = get_orthogonalized_matrix(block.self_attn.o_proj.weight.data.T, direction).T\n",
    "        block.mlp.down_proj.weight.data = get_orthogonalized_matrix(block.mlp.down_proj.weight.data.T, direction).T\n",
    "\n",
    "def get_orthogonalized_matrix(matrix: Float[Tensor, '... d_model'], vec: Float[Tensor, 'd_model']) -> Float[Tensor, '... d_model']:\n",
    "    vec = vec / torch.norm(vec)\n",
    "    vec = vec.to(matrix)\n",
    "\n",
    "    proj = einops.einsum(matrix, vec.unsqueeze(-1), '... d_model, d_model single -> ... single') * vec\n",
    "    return matrix - proj\n",
    "\n",
    "def act_add_gemma_weights(model, direction: Float[Tensor, \"d_model\"], coeff, layer):\n",
    "    dtype = model.model.layers[layer-1].mlp.down_proj.weight.dtype\n",
    "    device = model.model.layers[layer-1].mlp.down_proj.weight.device\n",
    "\n",
    "    bias = (coeff * direction).to(dtype=dtype, device=device)\n",
    "\n",
    "    model.model.layers[layer-1].mlp.down_proj.bias = torch.nn.Parameter(bias)\n",
    "\n",
    "class GemmaModel(ModelBase):\n",
    "\n",
    "    def _load_model(self, model_path, dtype=torch.bfloat16):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"cuda\",\n",
    "        ).eval()\n",
    "\n",
    "        model.requires_grad_(False) \n",
    "\n",
    "        return model\n",
    "\n",
    "    def _load_tokenizer(self, model_path):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        tokenizer.padding_side = 'left'\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def _get_tokenize_instructions_fn(self):\n",
    "        return functools.partial(tokenize_instructions_gemma_chat, tokenizer=self.tokenizer, system=None, include_trailing_whitespace=True)\n",
    "\n",
    "    def _get_eoi_toks(self):\n",
    "        return self.tokenizer.encode(GEMMA_CHAT_TEMPLATE.split(\"{instruction}\")[-1], add_special_tokens=False)\n",
    "\n",
    "    def _get_refusal_toks(self):\n",
    "        return GEMMA_REFUSAL_TOKS\n",
    "\n",
    "    def _get_model_block_modules(self):\n",
    "        return self.model.model.layers\n",
    "\n",
    "    def _get_attn_modules(self):\n",
    "        return torch.nn.ModuleList([block_module.self_attn for block_module in self.model_block_modules])\n",
    "    \n",
    "    def _get_mlp_modules(self):\n",
    "        return torch.nn.ModuleList([block_module.mlp for block_module in self.model_block_modules])\n",
    "\n",
    "    def _get_orthogonalization_mod_fn(self, direction: Float[Tensor, \"d_model\"]):\n",
    "        return functools.partial(orthogonalize_gemma_weights, direction=direction)\n",
    "    \n",
    "    def _get_act_add_mod_fn(self, direction: Float[Tensor, \"d_model\"], coeff, layer):\n",
    "        return functools.partial(act_add_gemma_weights, direction=direction, coeff=coeff, layer=layer)\n",
    "\n",
    "\n",
    "def get_mean_activations_pre_hook(layer, cache: Float[Tensor, \"pos layer d_model\"], n_samples, positions: List[int]):\n",
    "    def hook_fn(module, input):\n",
    "        activation: Float[Tensor, \"batch_size seq_len d_model\"] = input[0].clone().to(cache)\n",
    "        cache[:, layer] += (1.0 / n_samples) * activation[:, positions, :].sum(dim=0)\n",
    "    return hook_fn\n",
    "\n",
    "def get_mean_activations(model, tokenizer, instructions, tokenize_instructions_fn, block_modules: List[torch.nn.Module], batch_size=32, positions=[-1]):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    n_positions = len(positions)\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    n_samples = len(instructions)\n",
    "    d_model = model.config.hidden_size\n",
    "\n",
    "    mean_activations = torch.zeros((n_positions, n_layers, d_model), dtype=torch.float64, device=model.device)\n",
    "\n",
    "    fwd_pre_hooks = [(block_modules[layer], get_mean_activations_pre_hook(layer=layer, cache=mean_activations, n_samples=n_samples, positions=positions)) for layer in range(n_layers)]\n",
    "\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        inputs = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "\n",
    "        with add_hooks(module_forward_pre_hooks=fwd_pre_hooks, module_forward_hooks=[]):\n",
    "            model(\n",
    "                input_ids=inputs.input_ids.to(model.device),\n",
    "                attention_mask=inputs.attention_mask.to(model.device),\n",
    "            )\n",
    "\n",
    "    return mean_activations\n",
    "\n",
    "def get_mean_diff(model, tokenizer, harmful_instructions, harmless_instructions, tokenize_instructions_fn, block_modules: List[torch.nn.Module], batch_size=32, positions=[-1]):\n",
    "    mean_activations_harmful = get_mean_activations(model, tokenizer, harmful_instructions, tokenize_instructions_fn, block_modules, batch_size=batch_size, positions=positions)\n",
    "    mean_activations_harmless = get_mean_activations(model, tokenizer, harmless_instructions, tokenize_instructions_fn, block_modules, batch_size=batch_size, positions=positions)\n",
    "\n",
    "    mean_diff: Float[Tensor, \"n_positions n_layers d_model\"] = mean_activations_harmful - mean_activations_harmless\n",
    "\n",
    "    return mean_diff\n",
    "\n",
    "def generate_directions(model_base: ModelBase, harmful_instructions, harmless_instructions, artifact_dir):\n",
    "    if not os.path.exists(artifact_dir):\n",
    "        os.makedirs(artifact_dir)\n",
    "\n",
    "    mean_diffs = get_mean_diff(model_base.model, model_base.tokenizer, harmful_instructions, harmless_instructions, model_base.tokenize_instructions_fn, model_base.model_block_modules, positions=list(range(-len(model_base.eoi_toks), 0)))\n",
    "\n",
    "    assert mean_diffs.shape == (len(model_base.eoi_toks), model_base.model.config.num_hidden_layers, model_base.model.config.hidden_size)\n",
    "    assert not mean_diffs.isnan().any()\n",
    "\n",
    "    torch.save(mean_diffs, f\"{artifact_dir}/mean_diffs.pt\")\n",
    "\n",
    "    return mean_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea97a8e-4202-4763-a04b-e1a6d8fb5987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c968f79429452f8cab9d950553a196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 18\n",
      "Hidden size: 2048\n"
     ]
    }
   ],
   "source": [
    "# Load model gemma 2b it.\n",
    "\n",
    "MODEL_PATH = \"google/gemma-2b-it\"\n",
    "print(f\"Loading {MODEL_PATH}...\")\n",
    "model_base = GemmaModel(MODEL_PATH)\n",
    "print(f\"Number of layers: {model_base.model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model_base.model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0b21d2-51f9-47fd-bdc4-bdc0bfdf9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt, fwd_pre_hooks=[], fwd_hooks=[], max_new_tokens=50):\n",
    "    tokenized = model_base.tokenize_instructions_fn(instructions=[prompt])\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=model_base.tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    with add_hooks(module_forward_pre_hooks=fwd_pre_hooks,\n",
    "                  module_forward_hooks=fwd_hooks):\n",
    "        output = model_base.model.generate(\n",
    "            input_ids=tokenized.input_ids.to(model_base.model.device),\n",
    "            attention_mask=tokenized.attention_mask.to(model_base.model.device),\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "    \n",
    "    new_tokens = output[0, tokenized.input_ids.shape[1]:]\n",
    "    completion = model_base.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecb3274-7ee5-4dc4-adb6-20101774001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_steering_direction_for_itergen(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    yay_texts: List[str], \n",
    "    nay_texts: List[str],\n",
    "    tokenize_fn,\n",
    "    block_modules\n",
    ") -> torch.Tensor:\n",
    "    mean_diffs = get_mean_diff(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        harmful_instructions=yay_texts,\n",
    "        harmless_instructions=nay_texts,\n",
    "        tokenize_instructions_fn=tokenize_fn,\n",
    "        block_modules=block_modules,\n",
    "        positions=[-1]\n",
    "    )\n",
    "    \n",
    "    steering_dir = mean_diffs.mean(dim=(0, 1))\n",
    "    steering_dir = steering_dir / (steering_dir.norm() + 1e-8)\n",
    "    \n",
    "    return steering_dir.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139641bc-541b-4fe3-a14d-b9139a722461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Dict, List\n",
    "\n",
    "# indeed, very not good looking patch...\n",
    "import itergen.main\n",
    "from transformers import LogitsProcessorList\n",
    "from transformers.generation.logits_process import (\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper, \n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "\n",
    "def patched_update_gen_args(self, **gen_args):\n",
    "    self.generation_config.update(**gen_args)\n",
    "    warpers = LogitsProcessorList()    \n",
    "    if hasattr(self.generation_config, 'temperature') and self.generation_config.temperature is not None and self.generation_config.temperature != 1.0:\n",
    "        warpers.append(TemperatureLogitsWarper(self.generation_config.temperature))    \n",
    "    if hasattr(self.generation_config, 'top_k') and self.generation_config.top_k is not None and self.generation_config.top_k != 0:\n",
    "        warpers.append(TopKLogitsWarper(top_k=self.generation_config.top_k, min_tokens_to_keep=1))    \n",
    "    if hasattr(self.generation_config, 'top_p') and self.generation_config.top_p is not None and self.generation_config.top_p < 1.0:\n",
    "        warpers.append(TopPLogitsWarper(top_p=self.generation_config.top_p, min_tokens_to_keep=1))\n",
    "    self.logit_warper = warpers\n",
    "itergen.main.IterGen.update_gen_args = patched_update_gen_args\n",
    "\n",
    "from typing import Optional, Any, Dict, List\n",
    "from itergen.main import IterGen\n",
    "\n",
    "class IterGenS(IterGen):\n",
    "    def __init__(self, grammar: str, model_id: str, **kwargs):\n",
    "        super().__init__(grammar=grammar, model_id=model_id, **kwargs)\n",
    "        self.resample_count = 0\n",
    "    \n",
    "    def start(self, prompt):\n",
    "        super().start(prompt)\n",
    "        self.resample_count = 0\n",
    "    \n",
    "    def _get_next_token_grammar(self, gen_mode, next_token_scores, parse_results):\n",
    "        next_token, next_token_probs = self._get_next_token(\n",
    "            gen_mode, self.session_tokens, self.logit_warper, next_token_scores\n",
    "        )\n",
    "        \n",
    "        invalid_at_least_once = False\n",
    "        for idx in range(self.num_outputs):\n",
    "            is_valid = self._is_valid(idx, self.session_tokens[idx], next_token[idx])\n",
    "            \n",
    "            if not is_valid:\n",
    "                invalid_at_least_once = True\n",
    "                self.resample_count += 1\n",
    "                mask = self.dfa_mask_store.get_accept_mask(parse_results[idx])\n",
    "                next_token_scores = self._apply_mask(idx, mask, next_token_scores)\n",
    "        \n",
    "        if invalid_at_least_once:\n",
    "            next_token, next_token_probs = self._get_next_token(\n",
    "                gen_mode, self.session_tokens, self.logit_warper, next_token_scores\n",
    "            )\n",
    "        \n",
    "        return next_token, next_token_probs\n",
    "    \n",
    "    def view_stats(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'total_tokens': self._metadata.get('total_tokens', 0),\n",
    "            'total_resamples': self.resample_count,\n",
    "            'resample_rate': self.resample_count / max(1, self._metadata.get('total_tokens', 1)),\n",
    "            'steering_active': False,\n",
    "        }\n",
    "\n",
    "\n",
    "class IterGenDualS(IterGenS):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grammar: str,\n",
    "        model_id: str,\n",
    "        nay_ds: Optional[List[str]] = None,\n",
    "        yay_ds: Optional[List[str]] = None,\n",
    "        steering_coeff: float = 1.0,\n",
    "        steering_layers: Optional[List[int]] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if (nay_ds is None) != (yay_ds is None):\n",
    "            raise ValueError(\"Both nay_ds and yay_ds must be provided together, or both None\")\n",
    "        \n",
    "        self.steering_active = nay_ds is not None and yay_ds is not None\n",
    "        self.steering_coeff = steering_coeff\n",
    "        self.steering_direction = None\n",
    "        self._hook_handles = []\n",
    "        \n",
    "        super().__init__(grammar=grammar, model_id=model_id, **kwargs)\n",
    "        \n",
    "        if self.steering_active:\n",
    "            print(\"Extracting steering direction...\")\n",
    "            def simple_tokenize_fn(instructions):\n",
    "                return self.tokenizer(\n",
    "                    instructions,\n",
    "                    padding=True,\n",
    "                    truncation=False,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "            \n",
    "            self.steering_direction = extract_steering_direction_for_itergen(\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                yay_texts=yay_ds,\n",
    "                nay_texts=nay_ds,\n",
    "                tokenize_fn=simple_tokenize_fn,\n",
    "                block_modules=self.model.model.layers\n",
    "            )\n",
    "            \n",
    "            n_layers = self.model.config.num_hidden_layers\n",
    "            if steering_layers is None:\n",
    "                self.steering_layers = list(range(n_layers // 2, n_layers))\n",
    "            else:\n",
    "                self.steering_layers = steering_layers\n",
    "            \n",
    "            print(f\"Steering on layers: {self.steering_layers}\")\n",
    "    \n",
    "    def _apply_steering_hooks(self):\n",
    "        if not self.steering_active or self.steering_direction is None:\n",
    "            return\n",
    "        \n",
    "        self._remove_steering_hooks()\n",
    "        \n",
    "        for layer_idx in self.steering_layers:\n",
    "            layer = self.model.model.layers[layer_idx]\n",
    "            hook = get_activation_addition_input_pre_hook(\n",
    "                vector=self.steering_direction,\n",
    "                coeff=self.steering_coeff\n",
    "            )\n",
    "            handle = layer.register_forward_pre_hook(hook)\n",
    "            self._hook_handles.append(handle)\n",
    "    \n",
    "    def _remove_steering_hooks(self):\n",
    "        for handle in self._hook_handles:\n",
    "            handle.remove()\n",
    "        self._hook_handles = []\n",
    "    \n",
    "    def start(self, prompt):\n",
    "        super().start(prompt)\n",
    "        \n",
    "        if self.steering_active:\n",
    "            self._apply_steering_hooks()\n",
    "    \n",
    "    def view_stats(self) -> Dict[str, Any]:\n",
    "        stats = super().view_stats()\n",
    "        stats['steering_active'] = self.steering_active\n",
    "        return stats\n",
    "    \n",
    "    def __del__(self):\n",
    "        self._remove_steering_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c9d880-813c-458d-9639-a1bbf4faf4e2",
   "metadata": {},
   "source": [
    "Case 1: Simple Sentence Grammar - lower case a-z and space ended by period.\n",
    "\n",
    "> The yay and nay datasets are from the refusal_direction steering method, the sets of prompts that brings out desired and undesired behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c67473-6478-4f5a-85a2-8902702a68bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9232e0ba286d40649bb41cc453afc6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  there. \n",
      "Tokens: 3\n",
      "Resamples: 3\n",
      "With steering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5416528d923543eba6eabf368def91ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting steering direction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering on layers: [14, 15, 16, 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  world in a new language. \n",
      "Tokens: 7\n",
      "Resamples: 1\n",
      "Comparison\n",
      "Baseline resamples: 3\n",
      "Dual resamples: 1\n",
      "Reduction: 2\n"
     ]
    }
   ],
   "source": [
    "from itergen.main import IterGen\n",
    "\n",
    "grammar = \"\"\"\n",
    "start: sentence\n",
    "sentence: word+ \".\"\n",
    "word: /[a-z]+/\n",
    "%ignore \" \"\n",
    "\"\"\"\n",
    "\n",
    "yay_valid = [\"hello world\", \"the cat sat\", \"dogs bark\"]\n",
    "nay_invalid = [\"Hello World!\", \"The Cat Sat?\", \"Dogs123\"]\n",
    "prompt = \"hello\"\n",
    "print(\"Baseline\")\n",
    "\n",
    "baseline = IterGenS(grammar=grammar, model_id=MODEL_PATH)\n",
    "baseline.start(prompt)\n",
    "out_baseline = baseline.forward(stop_symbol=\"sentence\", num=1)\n",
    "stats_baseline = baseline.view_stats()\n",
    "\n",
    "print(f\"Output: {out_baseline[0]}\")\n",
    "print(f\"Tokens: {stats_baseline['total_tokens']}\")\n",
    "print(f\"Resamples: {stats_baseline['total_resamples']}\")\n",
    "\n",
    "print(\"With steering\")\n",
    "\n",
    "dual = IterGenDualS(\n",
    "    grammar=grammar,\n",
    "    model_id=MODEL_PATH,\n",
    "    nay_ds=nay_invalid,\n",
    "    yay_ds=yay_valid,\n",
    "    steering_coeff=2.0,\n",
    "    steering_layers=list(range(14, 18))\n",
    ")\n",
    "\n",
    "dual.start(prompt)\n",
    "out_dual = dual.forward(stop_symbol=\"sentence\", num=1)\n",
    "stats_dual = dual.view_stats()\n",
    "\n",
    "print(f\"Output: {out_dual[0]}\")\n",
    "print(f\"Tokens: {stats_dual['total_tokens']}\")\n",
    "print(f\"Resamples: {stats_dual['total_resamples']}\")\n",
    "\n",
    "print(\"Comparison\")\n",
    "print(f\"Baseline resamples: {stats_baseline['total_resamples']}\")\n",
    "print(f\"Dual resamples: {stats_dual['total_resamples']}\")\n",
    "print(f\"Reduction: {stats_baseline['total_resamples'] - stats_dual['total_resamples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22a935-3153-4530-aac4-b53825495ed5",
   "metadata": {},
   "source": [
    "The example was done in the manner of base models instead of using it as a instruction model with the system and user prompt special token so it has more room to be steered. In this setting the subsequent sequences will model the input, so the yay and nay sets of prompts can be used to elicit a steering vector that works on the output. Then we use a totally unrelated start (hello) and it did remove the amount of resamples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e47e7-651c-44bf-8c45-44ecbdda3d3e",
   "metadata": {},
   "source": [
    "Case 2: Simplified Case of the Purring Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ff55b6-5fe4-4588-97a8-f9a86e046266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848d4076be8a46c99adb948b0d24e45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:      nyaaaaahhhh the cat is a great household animal as it helps to remove rodents nya.\n",
      "Tokens: 25\n",
      "Resamples: 11\n",
      "With steering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7f23f37cac4d3c91a7e82139e81306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting steering direction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering on layers: [14, 15, 16, 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  ya know what i mean right now that the cat is a great household animal as it helps to remove rodents nya.\n",
      "Tokens: 25\n",
      "Resamples: 6\n",
      "Comparison\n",
      "Baseline resamples: 11\n",
      "Dual resamples: 6\n",
      "Reduction: 5\n"
     ]
    }
   ],
   "source": [
    "from itergen.main import IterGen\n",
    "\n",
    "grammar = \"\"\"\n",
    "start: sentence\n",
    "sentence: word+ \"nya\" \".\"\n",
    "word: /[a-z]+/\n",
    "%ignore \" \"\n",
    "\"\"\"\n",
    "\n",
    "nay_invalid = [\"the cat is on the mat.\", \"time spent with a cat is never wasted.\", \"curiosity killed the cat.\"]\n",
    "yay_valid = [\"the cat is on the mat nya.\", \"time spent with a cat is never wasted nya.\", \"curiosity killed the cat nya.\"]\n",
    "prompt = \"hello the cat is a great household animal as it helps to remove rodents nya. please continue writing this ending sentences with  nya.\"\n",
    "print(\"Baseline\")\n",
    "\n",
    "baseline = IterGenS(grammar=grammar, model_id=MODEL_PATH)\n",
    "baseline.start(prompt)\n",
    "out_baseline = baseline.forward(stop_symbol=\"sentence\", num=1)\n",
    "stats_baseline = baseline.view_stats()\n",
    "\n",
    "print(f\"Output: {out_baseline[0]}\")\n",
    "print(f\"Tokens: {stats_baseline['total_tokens']}\")\n",
    "print(f\"Resamples: {stats_baseline['total_resamples']}\")\n",
    "\n",
    "print(\"With steering\")\n",
    "dual = IterGenDualS(\n",
    "    grammar=grammar,\n",
    "    model_id=MODEL_PATH,\n",
    "    nay_ds=nay_invalid,\n",
    "    yay_ds=yay_valid,\n",
    "    steering_coeff=2.0,\n",
    "    steering_layers=list(range(14, 18))\n",
    ")\n",
    "\n",
    "dual.start(prompt)\n",
    "out_dual = dual.forward(stop_symbol=\"sentence\", num=1)\n",
    "stats_dual = dual.view_stats()\n",
    "print(f\"Output: {out_dual[0]}\")\n",
    "print(f\"Tokens: {stats_dual['total_tokens']}\")\n",
    "print(f\"Resamples: {stats_dual['total_resamples']}\")\n",
    "\n",
    "print(\"Comparison\")\n",
    "print(f\"Baseline resamples: {stats_baseline['total_resamples']}\")\n",
    "print(f\"Dual resamples: {stats_dual['total_resamples']}\")\n",
    "print(f\"Reduction: {stats_baseline['total_resamples'] - stats_dual['total_resamples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d7255-5f33-49f7-8335-b50c1addcc8d",
   "metadata": {},
   "source": [
    "For simplicity we only use one literal token (nya) and remove emojis since they might not fit in the grammar. Following 3(c) we also introduce the topic of cats in the prompt for generation, and using the same logic as case 1 we added the token nya to the yay dataset so the steering vector can capture the behavior of using purr tokens. This example shows the steering vector reducing the number of resamples needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e51549-45bb-4cda-be50-ebd7d534fd5a",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "Case 1 and 2 shows that the methods of steering from refusal_direction can work together with IterGen to reduce the number of resampling, although we only tested on one simplified case of \"English text EBNF grammar\" and another simplified of \"purring behavior\" from the previous question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c422033-09e4-420d-b12f-056b8996f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
