{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae306af-34a7-416b-bb45-0d3808e7d972",
   "metadata": {},
   "source": [
    "# CS 521 Homework 1 Problem 1\n",
    "Zhaochen Hong (zhong42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d976f4-c7e0-4b8f-b7d7-acf95a667f04",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddce8210-7b92-4d1c-b101-828f5fcd08cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af79925-f0f9-4266-b5a8-4eb9a34b967b",
   "metadata": {},
   "source": [
    "# Problem 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d246823f-b6e2-4ff3-8d80-ac136e5fe6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class:  2\n",
      "New Class:  0\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# fix seed so that random initialization always performs the same \n",
    "torch.manual_seed(13)\n",
    "\n",
    "\n",
    "# create the model N as described in the question\n",
    "N = nn.Sequential(nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 3, bias=False))\n",
    "\n",
    "# random input\n",
    "x = torch.rand((1,10)) # the first dimension is the batch size; the following dimensions the actual dimension of the data\n",
    "x.requires_grad_() # this is required so we can compute the gradient w.r.t x\n",
    "\n",
    "t = 0 # target class\n",
    "\n",
    "epsReal = 0.5  #depending on your data this might be large or small\n",
    "eps = epsReal - 1e-7 # small constant to offset floating-point erros\n",
    "\n",
    "# The network N classfies x as belonging to class 2\n",
    "original_class = N(x).argmax(dim=1).item()  # TO LEARN: make sure you understand this expression\n",
    "print(\"Original Class: \", original_class)\n",
    "assert(original_class == 2)\n",
    "\n",
    "# compute gradient\n",
    "# note that CrossEntropyLoss() combines the cross-entropy loss and an implicit softmax function\n",
    "L = nn.CrossEntropyLoss()\n",
    "loss = L(N(x), torch.tensor([t], dtype=torch.long)) # TO LEARN: make sure you understand this line\n",
    "loss.backward()\n",
    "\n",
    "# your code here\n",
    "# adv_x should be computed from x according to the fgsm-style perturbation such that the new class of xBar is the target class t above\n",
    "# hint: you can compute the gradient of the loss w.r.t to x as x.grad\n",
    "grad = x.grad\n",
    "sign_grad = grad.sign()\n",
    "adv_x = x - eps * sign_grad\n",
    "\n",
    "new_class = N(adv_x).argmax(dim=1).item()\n",
    "print(\"New Class: \", new_class)\n",
    "assert(new_class == t)\n",
    "# it is not enough that adv_x is classified as t. We also need to make sure it is 'close' to the original x. \n",
    "print(torch.norm((x-adv_x),  p=float('inf')).data)\n",
    "assert( torch.norm((x-adv_x), p=float('inf')) <= epsReal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65622bd-9b71-49c8-8402-87c8cdd93b4e",
   "metadata": {},
   "source": [
    "# Problem 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd89f3-cc97-4c8d-be2f-560acbb7c52f",
   "metadata": {},
   "source": [
    "I am using \"+ eps * sign_grad\" as the loss is computed for the classification to be t, and in reverse to attacking the model to not classify into t, I am encouraging it to classify into t, hence it's \"-\" instead of \"+\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a601e1ef-c345-481f-8d71-81d3851e0da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class:  2\n",
      "New Class:  1\n",
      "tensor(1.2000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# fix seed so that random initialization always performs the same \n",
    "torch.manual_seed(13)\n",
    "\n",
    "\n",
    "# create the model N as described in the question\n",
    "N = nn.Sequential(nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 10, bias=False),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(10, 3, bias=False))\n",
    "\n",
    "# random input\n",
    "x = torch.rand((1,10)) # the first dimension is the batch size; the following dimensions the actual dimension of the data\n",
    "x.requires_grad_() # this is required so we can compute the gradient w.r.t x\n",
    "\n",
    "t = 1 # target class\n",
    "\n",
    "epsReal = 1.2 #depending on your data this might be large or small\n",
    "eps = epsReal - 1e-7 # small constant to offset floating-point erros\n",
    "\n",
    "# The network N classfies x as belonging to class 2\n",
    "original_class = N(x).argmax(dim=1).item()  # TO LEARN: make sure you understand this expression\n",
    "print(\"Original Class: \", original_class)\n",
    "assert(original_class == 2)\n",
    "\n",
    "# compute gradient\n",
    "# note that CrossEntropyLoss() combines the cross-entropy loss and an implicit softmax function\n",
    "L = nn.CrossEntropyLoss()\n",
    "loss = L(N(x), torch.tensor([t], dtype=torch.long)) # TO LEARN: make sure you understand this line\n",
    "loss.backward()\n",
    "\n",
    "# your code here\n",
    "# adv_x should be computed from x according to the fgsm-style perturbation such that the new class of xBar is the target class t above\n",
    "# hint: you can compute the gradient of the loss w.r.t to x as x.grad\n",
    "grad = x.grad\n",
    "sign_grad = grad.sign()\n",
    "adv_x = x - eps * sign_grad\n",
    "\n",
    "new_class = N(adv_x).argmax(dim=1).item()\n",
    "print(\"New Class: \", new_class)\n",
    "assert(new_class == t)\n",
    "# it is not enough that adv_x is classified as t. We also need to make sure it is 'close' to the original x. \n",
    "print(torch.norm((x-adv_x),  p=float('inf')).data)\n",
    "assert( torch.norm((x-adv_x), p=float('inf')) <= epsReal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f76ac5-5c06-4960-bdf3-9e36326d4ba0",
   "metadata": {},
   "source": [
    "I attempted a range of epsilon values starting from 0.0, which does not guide the model to classify into the class of 1, to arbritrairly large values, which will induce so large perturbations that they overshadow x and cause the classification to be something other than the original class and the intended class 1 (in my case, class 0). Finally the epsilon value was obtained by testing out in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7168a-fbdb-41c8-91e1-a229161dd903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
